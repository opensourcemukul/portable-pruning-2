# ðŸ”§ Portable Pruning

**Data-Free. Device-Agnostic. Simple CNN Pruning for Edge Devices.**

This is a modular and clean framework to help you experiment with pruning deep learning models like `ResNet18`, `ResNet34`, and `MobileNetV2`. The best part â€” you donâ€™t need any training data for pruning. Itâ€™s built for people who want to deploy compressed models on edge devices like Raspberry Pi, Jetson Nano, or Android phones.

---

## ðŸ” What it Can Do

- âœ… Works without any training data
- ðŸ§± Device-agnostic â€” models run on various edge hardware
- ðŸ”„ Easy-to-plug pruning strategies
- ðŸ“ Measures: FLOPs, Params, Latency, RAM, Accuracy
- ðŸ“¤ Exports `.pth` and `.onnx` for deployment
- ðŸ“Š Logs every experiment in a CSV automatically

---

## âš™ï¸ Installation & Setup

### ðŸ”¹ Step 1: Clone the Repo

```bash
git clone https://github.com/yourusername/portable-pruning.git
cd portable-pruning
```

### ðŸ”¹ Step 2: Install Requirements

> This will install PyTorch, ONNX, and any helper libraries.

```bash
pip install -r requirements.txt
```

> If `portable_pruning/` is not being found as a module, you can either install it or add it to your PYTHONPATH.

```bash
# Optional
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
```

### ðŸ”¹ Step 3: Dataset

By default, it uses the `imagenette2-320` dataset. You can change this in the script if needed. Make sure the dataset is downloaded or accessible.

> No need to train anything â€” just prune and evaluate.

---

## ðŸ§  Models Available

- `resnet18` â€” lightweight and fast
- `resnet34` â€” deeper but still efficient
- `mobilenet_v2` â€” made for mobile deployment

All models load from `torchvision.models` with pretrained weights.

---

## âœ‚ï¸ Pruning Methods

| Method      | Description |
|-------------|-------------|
| `l1`        | Unstructured weight pruning based on L1 norm |
| `batchnorm` | Prunes based on BatchNorm gamma values |
| `red`       | KDE-based redundancy filtering (tune `alpha`, `tau`) |
| `redpp`     | A simplified variant of RED |
| `builtin`   | PyTorchâ€™s own `nn.utils.prune` interface |
| `autodfp`   | Auto Deep Filter Pruning (RL-based) |

Modes like `l1_unstructured`, `ln_structured`, etc. are available under the `builtin` method.

---

## ðŸš€ Run It (Your Way)

### ðŸ”¸ Prune a Model

```bash
python run_pruning.py   --model resnet18   --method builtin   --mode l1_unstructured   --compression 0.5   --onnx   --output_dir .outputs/my_run
```

### ðŸ”¸ Just Evaluate Baseline (No Pruning)

```bash
python run_pruning.py   --model resnet18   --method builtin   --mode l1_unstructured   --compression 0.5   --baseline   --onnx   --output_dir .outputs/my_run
```

Youâ€™ll get `.pth`, `.onnx`, and a bunch of console logs like FLOPs, latency, RAM, etc.

---

## ðŸ§ª Run a Full Experiment Grid

Run everything in one shot (all models, compressions, etc.).

```bash
python experiment.py --subset_size 100 --results_file results.csv
```

> This is useful when you want a proper benchmark with all configurations.

Outputs go to:

- `.outputs/run_<timestamp>/`
- `results.csv`

---

## ðŸ“„ License

MIT License Â© 2025 **Purnendu Prabhat**

---

If you're looking to do quick, clean, and practical CNN pruning for deployment â€” this repo is a good starting point.